
Missingness
-----------

Missing data is one of the most common unforeseen issues in data
analysis. In a prediction problem, we can deal with missing predictor
variables by engineering features that are missingness-aware. In this
case, simple imputation methods may suffice. When a variable of interest
is missing, such as the response variable in prediction, these same
imputation methods may lead us to some strange conclusions. First, let's
go over some basics of missing data.

.. code:: python

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import statsmodels.api as sm

Let's begin our discussion of the different types of missing by
simulating a dataset with missingness. In the following, we have a
dataset with two variable where each entry of the first is not missing
with probability .84 and is missing otherwise. The second variable is
non-missing.

.. code:: python

    p = .84
    X = np.random.normal(0,1,(n,2))
    M = np.random.binomial(1,.84,n) == 0
    X[M,0] = np.nan # X0 is not missing with prob .84
    X_MCAR = pd.DataFrame(X)
    X_MCAR.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.327573</td>
          <td>-2.403688</td>
        </tr>
        <tr>
          <th>1</th>
          <td>-0.549736</td>
          <td>-0.537814</td>
        </tr>
        <tr>
          <th>2</th>
          <td>NaN</td>
          <td>-0.169579</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.047577</td>
          <td>-0.013752</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.808157</td>
          <td>0.234873</td>
        </tr>
      </tbody>
    </table>
    </div>



We can see that a missing value is encoded as a NaN in the DataFrame.
The missing value may be encoded differently depending on the column
type (datetimes encode missingness as NaT for example). For numerical
types this is ``np.nan``.

.. code:: python

    X_MCAR.describe()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>170.000000</td>
          <td>200.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>-0.176160</td>
          <td>-0.055610</td>
        </tr>
        <tr>
          <th>std</th>
          <td>1.019463</td>
          <td>0.949872</td>
        </tr>
        <tr>
          <th>min</th>
          <td>-2.460313</td>
          <td>-2.403688</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>-0.919884</td>
          <td>-0.616639</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>-0.242606</td>
          <td>-0.046619</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>0.515983</td>
          <td>0.634754</td>
        </tr>
        <tr>
          <th>max</th>
          <td>2.886748</td>
          <td>2.643257</td>
        </tr>
      </tbody>
    </table>
    </div>



We can see that there are many missing values because the ``count`` is
smaller in X0. All of statistics in the ``describe`` function other than
``count`` are computed with the missingness ignored. Typically, Pandas
ignores missingness by default when computing statistics and making
plots. We can expect that because the missingness is done randomly and
independently of the other variables then these statistics are unbiased
for the population statistics (if there was no missingness).

**Def.** When a variable is missing independently and at random, and the
missingness status is not dependent on any observed value, we say the
variable is *missing completely at random (MCAR)*. In this case variable
X0 is MCAR because the missingness is not dependent on X1.

Let's simulate from a model where the missingness is dependent on the
second variable, X1.

.. code:: python

    n = 200
    X = np.random.normal(0,1,(n,2))
    X[X[:,1] > 1.,0] = np.nan # X0 is missing if X1 > 1
    X_MAR = pd.DataFrame(X)
    X_MAR.describe()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>172.000000</td>
          <td>200.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>0.007135</td>
          <td>-0.012531</td>
        </tr>
        <tr>
          <th>std</th>
          <td>1.027917</td>
          <td>0.961955</td>
        </tr>
        <tr>
          <th>min</th>
          <td>-2.497748</td>
          <td>-2.568347</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>-0.744684</td>
          <td>-0.557767</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>-0.100491</td>
          <td>0.030946</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>0.622769</td>
          <td>0.551149</td>
        </tr>
        <tr>
          <th>max</th>
          <td>2.509379</td>
          <td>3.018901</td>
        </tr>
      </tbody>
    </table>
    </div>



We can see that ``count`` is similar and none of the statistics for
variable X0 have significantly changed. This is because X1 was drawn
independently from X0, and so the missingness mechanism is also
independent from X0. We will return to this dataset.

**Def.** When a variable is missing independently and at random, but the
missingness status is dependent on another observed variable, we say the
variable is *missing at random (MAR)*. In the case above, X0 is MAR
because the missingness is dependent on X1 but not explicitely on X0.

Let's simulate from another model where the missingness of X0 is
dependent on its value.

.. code:: python

    X = np.random.normal(0,1,(n,2))
    X[X[:,0] > 1.,0] = np.nan # X0 is missing if X0 > 1
    X_MNAR = pd.DataFrame(X)
    X_MNAR.describe()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>167.000000</td>
          <td>200.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>-0.270645</td>
          <td>-0.050537</td>
        </tr>
        <tr>
          <th>std</th>
          <td>0.816188</td>
          <td>0.972811</td>
        </tr>
        <tr>
          <th>min</th>
          <td>-2.974269</td>
          <td>-2.888389</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>-0.884813</td>
          <td>-0.690774</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>-0.151122</td>
          <td>-0.065065</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>0.310640</td>
          <td>0.704453</td>
        </tr>
        <tr>
          <th>max</th>
          <td>0.993096</td>
          <td>1.992330</td>
        </tr>
      </tbody>
    </table>
    </div>



We see that ``count`` is again similar to the previous statistics, but
the other statistics have changed dramatically. The mean is brought down
because we made the large values of X0 be missing, and this has likewise
reduced the ``max`` which is concentrating around 1.

**Def.** When a variable's missingness remains dependent on its latent
value, even after conditioning on other observed values, this is called
*missing not at random (MNAR)*. This happens when the missingness is
neither MAR or MCAR.

Looking at missingness in Pandas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There is typically not much we can do about MNAR data if the missing
variable is of interest. On the other hand, we can sometimes guess at
the missingness mechanism. For example, if we had looked at the
histograms of the data,

.. code:: python

    X_MNAR.hist()
    plt.show()



.. image:: output_12_0.png


we might notice that variable X0 has a cliff at 1. We may correctly
suspect that all values above 1 were cencored.

How might we distinguish between the ``X_MAR`` and the ``X_MCAR``
datasets if we did not already know how they were simulated? In Pandas,
we can test which entries are missing with ``isna, notna``.

.. code:: python

    X_MCAR.isna().head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>0</th>
          <th>1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>False</td>
          <td>False</td>
        </tr>
        <tr>
          <th>1</th>
          <td>False</td>
          <td>False</td>
        </tr>
        <tr>
          <th>2</th>
          <td>True</td>
          <td>False</td>
        </tr>
        <tr>
          <th>3</th>
          <td>False</td>
          <td>False</td>
        </tr>
        <tr>
          <th>4</th>
          <td>False</td>
          <td>False</td>
        </tr>
      </tbody>
    </table>
    </div>



This converted all of the entries to the True/False answer to "is this
value missing?". This is the preferred way to evaluate missingness
because in Python ``np.nan == np.nan`` actually evaluates to False, so
testing with expressions of this type doesn't work.

You should think of the result of ``isna`` as another variable that you
may want to include in your analysis. In prediction tasks we will often
include these variables when we do feature engineering, as we will see
later. For our purposes, we can see the difference between the ``X_MAR``
and ``X_MCAR`` data with this missingness indicator included.
Specifically, we want to see what the dependence is between the
missingness and other variables. We can do this by grouping by the
missingness and describing the other variable. The following function
adds the missingness indicator to a dataset for a variable, removes that
variable, groups by the missingness, and describes the remaining
variables.

.. code:: python

    def desc_missing(X,missingvar):
        """Describes the variables conditioned on the missingness"""
        X_mod = X.copy()
        X_mod['miss_ind'] = X_mod[missingvar].isna()
        del X_mod[missingvar]
        return X_mod.groupby('miss_ind').describe()

.. code:: python

    desc_missing(X_MCAR,0)




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead tr th {
            text-align: left;
        }
    
        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th colspan="8" halign="left">1</th>
        </tr>
        <tr>
          <th></th>
          <th>count</th>
          <th>mean</th>
          <th>std</th>
          <th>min</th>
          <th>25%</th>
          <th>50%</th>
          <th>75%</th>
          <th>max</th>
        </tr>
        <tr>
          <th>miss_ind</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>False</th>
          <td>170.0</td>
          <td>-0.059364</td>
          <td>0.939388</td>
          <td>-2.403688</td>
          <td>-0.577253</td>
          <td>-0.046619</td>
          <td>0.575143</td>
          <td>2.643257</td>
        </tr>
        <tr>
          <th>True</th>
          <td>30.0</td>
          <td>-0.034340</td>
          <td>1.023837</td>
          <td>-2.152632</td>
          <td>-0.632773</td>
          <td>0.029868</td>
          <td>0.724111</td>
          <td>1.795847</td>
        </tr>
      </tbody>
    </table>
    </div>



.. code:: python

    desc_missing(X_MAR,0)




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead tr th {
            text-align: left;
        }
    
        .dataframe thead tr:last-of-type th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr>
          <th></th>
          <th colspan="8" halign="left">1</th>
        </tr>
        <tr>
          <th></th>
          <th>count</th>
          <th>mean</th>
          <th>std</th>
          <th>min</th>
          <th>25%</th>
          <th>50%</th>
          <th>75%</th>
          <th>max</th>
        </tr>
        <tr>
          <th>miss_ind</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>False</th>
          <td>172.0</td>
          <td>-0.255640</td>
          <td>0.790491</td>
          <td>-2.568347</td>
          <td>-0.695704</td>
          <td>-0.102948</td>
          <td>0.379905</td>
          <td>0.982192</td>
        </tr>
        <tr>
          <th>True</th>
          <td>28.0</td>
          <td>1.480858</td>
          <td>0.416369</td>
          <td>1.069409</td>
          <td>1.218558</td>
          <td>1.365086</td>
          <td>1.586109</td>
          <td>3.018901</td>
        </tr>
      </tbody>
    </table>
    </div>



In the above displays, we see that for the MCAR dataset there is little
difference between the summary statistics for X1 when X0 is missing or
not. For the MAR dataset, there is a more pronounced difference between
the means of these two subsets of the data. This is one indication to us
that ``X_MAR`` is not MCAR.

We could also do this formally with a T-test for difference in means
between two samples to do this more formally. I will assume that you
recall the T-test for independence from your introductory statistics
course, if not please review it before trying to parse the following
code blocks.

.. code:: python

    def test_MCAR(X,missingvar,testvar):
        """
        Test if testvar is independent of missingness
        Uses two-sided t-test with pooled variances
        """
        X_mod = X.copy()
        X_mod['miss_ind'] = X_mod[missingvar].isna()
        del X_mod[missingvar]
        X1,X2 = [df for i,df in X_mod.groupby('miss_ind')] #split
        return sm.stats.ttest_ind(X1[testvar],X2[testvar]) #return test

In the above function we split the grouped dataframe based on the
missingness status. We used the ``ttest_ind`` method in statsmodels to
perform the two-sample t-test with pooled variances. The resulting
P-values are below:

.. code:: python

    print("X_MCAR P-value: {}".format(test_MCAR(X_MCAR,0,1)[1]))
    print("X_MAR P-value: {}".format(test_MCAR(X_MAR,0,1)[1]))


.. parsed-literal::

    X_MCAR P-value: 0.8945613701172417
    X_MAR P-value: 2.4792949402817898e-23


This provides more satisfying evidence that the dataset ``X_MAR`` is not
drawn MCAR.

Dealing with missingness and simple imputation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Imputation is the process of filling in missing values. There are few
great options when it comes to imputation, and very few general purpose
tools that will perfectly impute missing data. Let's discuss some
reasonable simple options and their limitations.

Let's begin by loading in the Melbourne housing dataset, which is from
the Kaggle competitions:
https://www.kaggle.com/anthonypino/melbourne-housing-market/version/26
The dataset describes home sales in Melborne, Australia.

.. code:: python

    mel = pd.read_csv('../data/Melbourne_housing_FULL.csv')
    mel.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Suburb</th>
          <th>Address</th>
          <th>Rooms</th>
          <th>Type</th>
          <th>Price</th>
          <th>Method</th>
          <th>SellerG</th>
          <th>Date</th>
          <th>Distance</th>
          <th>Postcode</th>
          <th>...</th>
          <th>Bathroom</th>
          <th>Car</th>
          <th>Landsize</th>
          <th>BuildingArea</th>
          <th>YearBuilt</th>
          <th>CouncilArea</th>
          <th>Lattitude</th>
          <th>Longtitude</th>
          <th>Regionname</th>
          <th>Propertycount</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>Abbotsford</td>
          <td>68 Studley St</td>
          <td>2</td>
          <td>h</td>
          <td>NaN</td>
          <td>SS</td>
          <td>Jellis</td>
          <td>3/09/2016</td>
          <td>2.5</td>
          <td>3067.0</td>
          <td>...</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>126.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Yarra City Council</td>
          <td>-37.8014</td>
          <td>144.9958</td>
          <td>Northern Metropolitan</td>
          <td>4019.0</td>
        </tr>
        <tr>
          <th>1</th>
          <td>Abbotsford</td>
          <td>85 Turner St</td>
          <td>2</td>
          <td>h</td>
          <td>1480000.0</td>
          <td>S</td>
          <td>Biggin</td>
          <td>3/12/2016</td>
          <td>2.5</td>
          <td>3067.0</td>
          <td>...</td>
          <td>1.0</td>
          <td>1.0</td>
          <td>202.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Yarra City Council</td>
          <td>-37.7996</td>
          <td>144.9984</td>
          <td>Northern Metropolitan</td>
          <td>4019.0</td>
        </tr>
        <tr>
          <th>2</th>
          <td>Abbotsford</td>
          <td>25 Bloomburg St</td>
          <td>2</td>
          <td>h</td>
          <td>1035000.0</td>
          <td>S</td>
          <td>Biggin</td>
          <td>4/02/2016</td>
          <td>2.5</td>
          <td>3067.0</td>
          <td>...</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>156.0</td>
          <td>79.0</td>
          <td>1900.0</td>
          <td>Yarra City Council</td>
          <td>-37.8079</td>
          <td>144.9934</td>
          <td>Northern Metropolitan</td>
          <td>4019.0</td>
        </tr>
        <tr>
          <th>3</th>
          <td>Abbotsford</td>
          <td>18/659 Victoria St</td>
          <td>3</td>
          <td>u</td>
          <td>NaN</td>
          <td>VB</td>
          <td>Rounds</td>
          <td>4/02/2016</td>
          <td>2.5</td>
          <td>3067.0</td>
          <td>...</td>
          <td>2.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>Yarra City Council</td>
          <td>-37.8114</td>
          <td>145.0116</td>
          <td>Northern Metropolitan</td>
          <td>4019.0</td>
        </tr>
        <tr>
          <th>4</th>
          <td>Abbotsford</td>
          <td>5 Charles St</td>
          <td>3</td>
          <td>h</td>
          <td>1465000.0</td>
          <td>SP</td>
          <td>Biggin</td>
          <td>4/03/2017</td>
          <td>2.5</td>
          <td>3067.0</td>
          <td>...</td>
          <td>2.0</td>
          <td>0.0</td>
          <td>134.0</td>
          <td>150.0</td>
          <td>1900.0</td>
          <td>Yarra City Council</td>
          <td>-37.8093</td>
          <td>144.9944</td>
          <td>Northern Metropolitan</td>
          <td>4019.0</td>
        </tr>
      </tbody>
    </table>
    <p>5 rows Ã— 21 columns</p>
    </div>



.. code:: python

    mel.info()


.. parsed-literal::

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 34857 entries, 0 to 34856
    Data columns (total 21 columns):
    Suburb           34857 non-null object
    Address          34857 non-null object
    Rooms            34857 non-null int64
    Type             34857 non-null object
    Price            27247 non-null float64
    Method           34857 non-null object
    SellerG          34857 non-null object
    Date             34857 non-null object
    Distance         34856 non-null float64
    Postcode         34856 non-null float64
    Bedroom2         26640 non-null float64
    Bathroom         26631 non-null float64
    Car              26129 non-null float64
    Landsize         23047 non-null float64
    BuildingArea     13742 non-null float64
    YearBuilt        15551 non-null float64
    CouncilArea      34854 non-null object
    Lattitude        26881 non-null float64
    Longtitude       26881 non-null float64
    Regionname       34854 non-null object
    Propertycount    34854 non-null float64
    dtypes: float64(12), int64(1), object(8)
    memory usage: 5.6+ MB


From the count we can see that there is significant missingness in many
of these variables. While we would like to impute as many of these
values as possible, we are especially interested in imputing the Price
of the home. Before we proceed with this, we can notice that Date and
YearBuilt columns are not datetimes, so we should convert these first.

.. code:: python

    mel['Date'][:5]




.. parsed-literal::

    0    3/09/2016
    1    3/12/2016
    2    4/02/2016
    3    4/02/2016
    4    4/03/2017
    Name: Date, dtype: object



Looking at the Date, we can see that the string is in a standard date
format, and we can then cast as a datetime.

.. code:: python

    mel['Date'] = pd.to_datetime(mel['Date'],format="%d/%m/%Y")

The Yearbuilt variable is a bit more challenging, since it was cast as a
float.

.. code:: python

    mel['YearBuilt'][:5]




.. parsed-literal::

    0       NaN
    1       NaN
    2    1900.0
    3       NaN
    4    1900.0
    Name: YearBuilt, dtype: float64



Let's write a custom function that converts the float first to integer,
then to string, if it is not NaN.

.. code:: python

    def conv_year(y):
        if np.isnan(y):
            return np.nan
        return str(int(y)) 

We apply this using the ``apply`` method, then convert to a PeriodIndex
object.

.. code:: python

    mel['YearBuilt'] = mel['YearBuilt'].apply(conv_year)
    mel['YearBuilt'] = pd.PeriodIndex(mel['YearBuilt'],freq="Y")

We can verify that this is the desired result.

.. code:: python

    mel['YearBuilt'][:5]




.. parsed-literal::

    0    NaT
    1    NaT
    2   1900
    3    NaT
    4   1900
    Name: YearBuilt, dtype: object



We have seen the use of ``isna`` to make a missingness indicator
variable. We can easily do this to all of the variables and add
missingness indicators for all of them using a join.

.. code:: python

    mel_miss = mel.isna()
    mel = mel.join(mel_miss,rsuffix="_miss")

Now ``mel`` has twice the number of columns.

.. code:: python

    mel.columns




.. parsed-literal::

    Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',
           'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',
           'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',
           'Longtitude', 'Regionname', 'Propertycount', 'Suburb_miss',
           'Address_miss', 'Rooms_miss', 'Type_miss', 'Price_miss', 'Method_miss',
           'SellerG_miss', 'Date_miss', 'Distance_miss', 'Postcode_miss',
           'Bedroom2_miss', 'Bathroom_miss', 'Car_miss', 'Landsize_miss',
           'BuildingArea_miss', 'YearBuilt_miss', 'CouncilArea_miss',
           'Lattitude_miss', 'Longtitude_miss', 'Regionname_miss',
           'Propertycount_miss'],
          dtype='object')



The simplest form of imputation is to replace the missing values in a
column with a single value. This can be accomplished with ``fillna``
which can take either a single value or a Series. The Series in this
case has to have keys which are the column names to be filled, and the
values are what to fill with.

For example, suppose that for all of the float columns we want to impute
with the median, and for all of the rest we want to impute with the
mode. We can use the ``select_dtypes`` to select the columns of a
certain type.

.. code:: python

    imp_values = mel.select_dtypes(exclude=float).mode().loc[0] # mode returns a DataFrame
    imp_values = imp_values.append(mel.select_dtypes(include=float).median())

We used ``append`` to combine the two resulting Series to give us the
following,

.. code:: python

    imp_values




.. parsed-literal::

    Suburb                              Reservoir
    Address                          5 Charles St
    Rooms                                       3
    Type                                        h
    Method                                      S
    SellerG                                Jellis
    Date                      2017-10-28 00:00:00
    YearBuilt                                1970
    CouncilArea           Boroondara City Council
    Regionname              Southern Metropolitan
    Suburb_miss                             False
    Address_miss                            False
    Rooms_miss                              False
    Type_miss                               False
    Price_miss                              False
    Method_miss                             False
    SellerG_miss                            False
    Date_miss                               False
    Distance_miss                           False
    Postcode_miss                           False
    Bedroom2_miss                           False
    Bathroom_miss                           False
    Car_miss                                False
    Landsize_miss                           False
    BuildingArea_miss                        True
    YearBuilt_miss                           True
    CouncilArea_miss                        False
    Lattitude_miss                          False
    Longtitude_miss                         False
    Regionname_miss                         False
    Propertycount_miss                      False
    Price                                  870000
    Distance                                 10.3
    Postcode                                 3103
    Bedroom2                                    3
    Bathroom                                    2
    Car                                         2
    Landsize                                  521
    BuildingArea                              136
    Lattitude                            -37.8076
    Longtitude                            145.008
    Propertycount                            6763
    dtype: object



Now we are prepared to impute with these values. We are only going to
impute with this simple scheme when CouncilArea is missing however,
which we will explain shortly.

.. code:: python

    ca_na = mel['CouncilArea'].isna()
    mel[ca_na] = mel[ca_na].fillna(imp_values)

We will impute using the CouncilArea as a predictor, but when it is
missing we will need to impute using a single value for each column.
There are only three rows with missing CouncilArea though, so this is
just for completeness sake. For comparison purposes however, we can make
a copy of the Price and look at the distribution before and after
imputation with a single value.

.. code:: python

    price = mel['Price'].copy()

We will first look at the histogram of Price when it is non-missing. We
can see from the imputed value above that the median is 870,000 AUD.

.. code:: python

    price.hist(bins=40)
    plt.title('Histogram of Price: non-missing')
    plt.show()



.. image:: output_52_0.png


Contrasting this with the histogram of the Price after we impute with
the median for the non-missing, we see one immediate deficiency of
imputing with a single value.

.. code:: python

    price.fillna(imp_values['Price']).hist(bins=40)
    plt.title('Histogram of Price: single value imputation')
    plt.show()



.. image:: output_54_0.png


The histogram now has a spike at the imputed value. This indicates that
if we impute with a single value then the sample distribution will have
a point mass. Imputing with the median will preserve the median, but
statistics regarding the spread of the distribution will be lower, for
example, compare the median and standard deviation below (50% and std).

.. code:: python

    price.describe()




.. parsed-literal::

    count    2.724700e+04
    mean     1.050173e+06
    std      6.414671e+05
    min      8.500000e+04
    25%      6.350000e+05
    50%      8.700000e+05
    75%      1.295000e+06
    max      1.120000e+07
    Name: Price, dtype: float64



.. code:: python

    price.fillna(imp_values['Price']).describe()




.. parsed-literal::

    count    3.485700e+04
    mean     1.010838e+06
    std      5.719992e+05
    min      8.500000e+04
    25%      6.950000e+05
    50%      8.700000e+05
    75%      1.150000e+06
    max      1.120000e+07
    Name: Price, dtype: float64



If we would like to get a more plausible sample distribution for the
Price after imputation, then we have two choices: multiple imputation
(which we will not go into) and imputation using prediction.

In Pandas, you can also use ``fillna`` to impute with a time series by
filling in the entries by padding from a given value. This type of
padding can be done in a forward or backward fashion. While we could use
Date to impute the Price, it will likely be more effective to use either
locational information or all of the continuous/ordinal variables
including Date.

Imputation by Prediction
~~~~~~~~~~~~~~~~~~~~~~~~

Recall that if a variable is MCAR then looking at the other variables
should not help the imputation process. If the Price is MAR then we can
impute by thinking of this as a prediction problem, with all other
variables and their missingness statuses as predictors and the Price as
a response.

We will first impute all of the variables using CouncilArea as the sole
predictor. We can simply think of this as grouping by the CouncilArea
and then imputing with a single value for that group and column.
CouncilArea is a good choice as a variable to condition on since it is
mostly non-missing, and as we will see in the next code block, when we
group by the CouncilArea there are no columns that are all missing in a
group. Hence, if we want to impute with the median or mode within a
group, this will always be well defined.

.. code:: python

    ca_groupct = mel.groupby('CouncilArea').count()
    (ca_groupct == 0).any().any()
    # is there any column that is all NaN for any council?




.. parsed-literal::

    False



In order to impute within groups we will combine the ``groupby`` method
with the ``transform``. The idea is that we want to make a
transformation within each group, and this can be accomplished in the
following way. But we first need to create a function that defines how
we want to impute within the groups (which is the tranformation that
needs to be applied).

.. code:: python

    def impute_med(x):
        """impute NaNs for Series x with median if it is a float and mode otherwise"""
        if x.dtype == float:
            return x.fillna(x.median())
        if x.name  == "YearBuilt":
            return x.fillna(x.mode()[0])
        return x.fillna(x.mode())

In the above transformation, we test if the Series has a float dtype,
and if so fill the NaNs with the median, otherwise we use the mode. The
YearBuild variable is causing some troubles because the mode returns a
Series and not a single value, so we will deal with that variable
specifically by looking at the ``name`` of the Series.

.. code:: python

    CA = mel['CouncilArea'].copy() # the process does not preserve CouncilArea, save it
    mel[~ca_na] = mel[~ca_na].groupby('CouncilArea').transform(impute_med)
    mel['CouncilArea'] = CA # rewrite CouncilArea

We can see that now all values have been imputed.

.. code:: python

    mel.info()


.. parsed-literal::

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 34857 entries, 0 to 34856
    Data columns (total 42 columns):
    Suburb                34857 non-null object
    Address               34857 non-null object
    Rooms                 34857 non-null int64
    Type                  34857 non-null object
    Price                 34857 non-null float64
    Method                34857 non-null object
    SellerG               34857 non-null object
    Date                  34857 non-null datetime64[ns]
    Distance              34857 non-null float64
    Postcode              34857 non-null float64
    Bedroom2              34857 non-null float64
    Bathroom              34857 non-null float64
    Car                   34857 non-null float64
    Landsize              34857 non-null float64
    BuildingArea          34857 non-null float64
    YearBuilt             34857 non-null object
    CouncilArea           34857 non-null object
    Lattitude             34857 non-null float64
    Longtitude            34857 non-null float64
    Regionname            34857 non-null object
    Propertycount         34857 non-null float64
    Suburb_miss           34857 non-null bool
    Address_miss          34857 non-null bool
    Rooms_miss            34857 non-null bool
    Type_miss             34857 non-null bool
    Price_miss            34857 non-null bool
    Method_miss           34857 non-null bool
    SellerG_miss          34857 non-null bool
    Date_miss             34857 non-null bool
    Distance_miss         34857 non-null bool
    Postcode_miss         34857 non-null bool
    Bedroom2_miss         34857 non-null bool
    Bathroom_miss         34857 non-null bool
    Car_miss              34857 non-null bool
    Landsize_miss         34857 non-null bool
    BuildingArea_miss     34857 non-null bool
    YearBuilt_miss        34857 non-null bool
    CouncilArea_miss      34857 non-null bool
    Lattitude_miss        34857 non-null bool
    Longtitude_miss       34857 non-null bool
    Regionname_miss       34857 non-null bool
    Propertycount_miss    34857 non-null bool
    dtypes: bool(21), datetime64[ns](1), float64(11), int64(1), object(8)
    memory usage: 6.3+ MB


Again, we can look at the histogram,

.. code:: python

    mel['Price'].hist(bins=40)
    plt.title("Histogram of Price: imputed by CouncilArea")
    plt.show()



.. image:: output_68_0.png


While there seems to be a larger spike around 1M AUD, the sample
distribution does look more like the original sample distribution.

We have seen how you can impute with a categorical variable as the
predictor using ``groupby``. We can also think about using the other
variables, particularly the numerical variables, in a linear regressor.
Before we do this however, we would like to compare prediction with the
numerical variables against predicting with CouncilArea. To do this
let's make a training and testing split of the observed Price data
(using the ``Price_miss`` variable). We will see the ``sklearn`` package
later, but for now, we will just import the ``train_test_split``
function. This makes a randomized split of the data with a default of a
3:1 ratio for train and test sets respectively.

.. code:: python

    from sklearn.model_selection import train_test_split
    
    train, test = train_test_split(mel[~mel['Price_miss']])

We will extract the predictions in the form of a Series with the
CouncilArea as the index and the mean Price within that CouncilArea as
the values.

.. code:: python

    ca_na = train['CouncilArea_miss']
    tr_preds = train[~ca_na][['CouncilArea','Price']].groupby('CouncilArea').mean()

We will then join this series to the full Price data with the
CouncilArea as the index. Notice that the left DataFrame is much larger
and has many duplicate indices, while the indices in the right DataFrame
are unique. Because this is a left join, the right Dataframe rows will
be multiplied to match the multiple rows on the left.

.. code:: python

    test_dat = test[['CouncilArea','Price']]
    test_dat = test_dat.set_index('CouncilArea')
    test_dat = test_dat.join(tr_preds,rsuffix='_pred')

We can look at the result and see that the behavior is as expected, the
``Price_pred`` is dependent on the CouncilArea.

.. code:: python

    test_dat.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Price</th>
          <th>Price_pred</th>
        </tr>
        <tr>
          <th>CouncilArea</th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Banyule City Council</th>
          <td>1278000.0</td>
          <td>942259.175601</td>
        </tr>
        <tr>
          <th>Banyule City Council</th>
          <td>845000.0</td>
          <td>942259.175601</td>
        </tr>
        <tr>
          <th>Banyule City Council</th>
          <td>985000.0</td>
          <td>942259.175601</td>
        </tr>
        <tr>
          <th>Banyule City Council</th>
          <td>690000.0</td>
          <td>942259.175601</td>
        </tr>
        <tr>
          <th>Banyule City Council</th>
          <td>728500.0</td>
          <td>942259.175601</td>
        </tr>
      </tbody>
    </table>
    </div>



We can simply calculate the square error test error using the ``eval``
method. We have not gone over the ``eval`` method, but it is often
handy. Since the columns are named, we can write mathematical
expressions using them, then ``eval`` will evaluate it for every row of
the DataFrame. We then calculate the mean.

.. code:: python

    CA_error = test_dat.eval('(Price - Price_pred)**2').mean()
    CA_error




.. parsed-literal::

    312621748714.5273



Let's consider using linear regression to predict the Price instead. We
will create a handy function to select the desired predictor and
response DataFrames below. We will exclude the non-numeric variables
from the predictors. Critically, the missingness indicators are all
included as predictor variables. We also cast the result as Numpy
Arrrays with dtype of float.

.. code:: python

    def select_preds(train):
        """"""
        X_tr = train.select_dtypes(include=[float,bool,int])
        Y_tr = X_tr['Price'].copy()
        del X_tr['Price']
        names = list(X_tr.columns)
        X_tr, Y_tr = X_tr.astype(float), Y_tr.astype(float)
        return Y_tr, X_tr

We then apply this to the train and test sets and fit OLS and predict on
the test set.

.. code:: python

    Y_tr, X_tr = select_preds(train)
    Y_te, X_te = select_preds(test)
    ols_res = sm.OLS(Y_tr,X_tr).fit()
    Y_pred = ols_res.predict(X_te)

.. code:: python

    ols_res.summary()




.. raw:: html

    <table class="simpletable">
    <caption>OLS Regression Results</caption>
    <tr>
      <th>Dep. Variable:</th>          <td>Price</td>      <th>  R-squared:         </th>  <td>   0.852</td>  
    </tr>
    <tr>
      <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.852</td>  
    </tr>
    <tr>
      <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   5871.</td>  
    </tr>
    <tr>
      <th>Date:</th>             <td>Thu, 20 Sep 2018</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   
    </tr>
    <tr>
      <th>Time:</th>                 <td>09:42:51</td>     <th>  Log-Likelihood:    </th> <td>-2.9610e+05</td>
    </tr>
    <tr>
      <th>No. Observations:</th>      <td> 20435</td>      <th>  AIC:               </th>  <td>5.922e+05</td> 
    </tr>
    <tr>
      <th>Df Residuals:</th>          <td> 20415</td>      <th>  BIC:               </th>  <td>5.924e+05</td> 
    </tr>
    <tr>
      <th>Df Model:</th>              <td>    20</td>      <th>                     </th>      <td> </td>     
    </tr>
    <tr>
      <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     
    </tr>
    </table>
    <table class="simpletable">
    <tr>
               <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
    </tr>
    <tr>
      <th>Rooms</th>              <td> 3.419e+05</td> <td> 6794.377</td> <td>   50.315</td> <td> 0.000</td> <td> 3.29e+05</td> <td> 3.55e+05</td>
    </tr>
    <tr>
      <th>Distance</th>           <td>-4.733e+04</td> <td>  604.181</td> <td>  -78.341</td> <td> 0.000</td> <td>-4.85e+04</td> <td>-4.61e+04</td>
    </tr>
    <tr>
      <th>Postcode</th>           <td> 1160.6889</td> <td>   35.744</td> <td>   32.472</td> <td> 0.000</td> <td> 1090.628</td> <td> 1230.750</td>
    </tr>
    <tr>
      <th>Bedroom2</th>           <td> -3.49e+04</td> <td> 8042.990</td> <td>   -4.339</td> <td> 0.000</td> <td>-5.07e+04</td> <td>-1.91e+04</td>
    </tr>
    <tr>
      <th>Bathroom</th>           <td> 1.741e+05</td> <td> 6443.674</td> <td>   27.020</td> <td> 0.000</td> <td> 1.61e+05</td> <td> 1.87e+05</td>
    </tr>
    <tr>
      <th>Car</th>                <td> 5.412e+04</td> <td> 4224.249</td> <td>   12.812</td> <td> 0.000</td> <td> 4.58e+04</td> <td> 6.24e+04</td>
    </tr>
    <tr>
      <th>Landsize</th>           <td>    4.1204</td> <td>    0.998</td> <td>    4.128</td> <td> 0.000</td> <td>    2.164</td> <td>    6.077</td>
    </tr>
    <tr>
      <th>BuildingArea</th>       <td>   60.9449</td> <td>   10.398</td> <td>    5.861</td> <td> 0.000</td> <td>   40.565</td> <td>   81.325</td>
    </tr>
    <tr>
      <th>Lattitude</th>          <td>-1.258e+06</td> <td> 4.03e+04</td> <td>  -31.204</td> <td> 0.000</td> <td>-1.34e+06</td> <td>-1.18e+06</td>
    </tr>
    <tr>
      <th>Longtitude</th>         <td>-3.509e+05</td> <td> 1.05e+04</td> <td>  -33.521</td> <td> 0.000</td> <td>-3.71e+05</td> <td> -3.3e+05</td>
    </tr>
    <tr>
      <th>Propertycount</th>      <td>   -1.3879</td> <td>    0.743</td> <td>   -1.868</td> <td> 0.062</td> <td>   -2.844</td> <td>    0.069</td>
    </tr>
    <tr>
      <th>Suburb_miss</th>        <td> 6.778e-08</td> <td> 2.01e-08</td> <td>    3.380</td> <td> 0.001</td> <td> 2.85e-08</td> <td> 1.07e-07</td>
    </tr>
    <tr>
      <th>Address_miss</th>       <td>-2.664e-07</td> <td> 7.85e-08</td> <td>   -3.395</td> <td> 0.001</td> <td> -4.2e-07</td> <td>-1.13e-07</td>
    </tr>
    <tr>
      <th>Rooms_miss</th>         <td> 4.365e-07</td> <td> 1.29e-07</td> <td>    3.395</td> <td> 0.001</td> <td> 1.84e-07</td> <td> 6.88e-07</td>
    </tr>
    <tr>
      <th>Type_miss</th>          <td>   5.8e-08</td> <td> 1.71e-08</td> <td>    3.395</td> <td> 0.001</td> <td> 2.45e-08</td> <td> 9.15e-08</td>
    </tr>
    <tr>
      <th>Price_miss</th>         <td> 1.287e-08</td> <td>  3.8e-09</td> <td>    3.390</td> <td> 0.001</td> <td> 5.43e-09</td> <td> 2.03e-08</td>
    </tr>
    <tr>
      <th>Method_miss</th>        <td>-2.877e-08</td> <td>  8.5e-09</td> <td>   -3.385</td> <td> 0.001</td> <td>-4.54e-08</td> <td>-1.21e-08</td>
    </tr>
    <tr>
      <th>SellerG_miss</th>       <td>-2.241e-08</td> <td> 6.49e-09</td> <td>   -3.452</td> <td> 0.001</td> <td>-3.51e-08</td> <td>-9.68e-09</td>
    </tr>
    <tr>
      <th>Date_miss</th>          <td>-1.267e-08</td> <td> 3.77e-09</td> <td>   -3.365</td> <td> 0.001</td> <td>-2.01e-08</td> <td>-5.29e-09</td>
    </tr>
    <tr>
      <th>Distance_miss</th>      <td> -1.75e+05</td> <td> 2.91e+05</td> <td>   -0.601</td> <td> 0.548</td> <td>-7.45e+05</td> <td> 3.95e+05</td>
    </tr>
    <tr>
      <th>Postcode_miss</th>      <td> -1.75e+05</td> <td> 2.91e+05</td> <td>   -0.601</td> <td> 0.548</td> <td>-7.45e+05</td> <td> 3.95e+05</td>
    </tr>
    <tr>
      <th>Bedroom2_miss</th>      <td>-1.244e+06</td> <td> 3.38e+05</td> <td>   -3.677</td> <td> 0.000</td> <td>-1.91e+06</td> <td>-5.81e+05</td>
    </tr>
    <tr>
      <th>Bathroom_miss</th>      <td> 1.109e+06</td> <td> 3.37e+05</td> <td>    3.289</td> <td> 0.001</td> <td> 4.48e+05</td> <td> 1.77e+06</td>
    </tr>
    <tr>
      <th>Car_miss</th>           <td> 7.084e+04</td> <td> 2.85e+04</td> <td>    2.484</td> <td> 0.013</td> <td>  1.5e+04</td> <td> 1.27e+05</td>
    </tr>
    <tr>
      <th>Landsize_miss</th>      <td>-1.822e+04</td> <td> 1.12e+04</td> <td>   -1.630</td> <td> 0.103</td> <td>-4.01e+04</td> <td> 3691.804</td>
    </tr>
    <tr>
      <th>BuildingArea_miss</th>  <td> 1.264e+04</td> <td> 1.25e+04</td> <td>    1.015</td> <td> 0.310</td> <td>-1.18e+04</td> <td> 3.71e+04</td>
    </tr>
    <tr>
      <th>YearBuilt_miss</th>     <td> 4.737e+04</td> <td> 1.26e+04</td> <td>    3.757</td> <td> 0.000</td> <td> 2.27e+04</td> <td> 7.21e+04</td>
    </tr>
    <tr>
      <th>CouncilArea_miss</th>   <td>-4.717e+04</td> <td> 1.12e+05</td> <td>   -0.421</td> <td> 0.674</td> <td>-2.67e+05</td> <td> 1.72e+05</td>
    </tr>
    <tr>
      <th>Lattitude_miss</th>     <td>-1.956e+04</td> <td> 1.89e+04</td> <td>   -1.034</td> <td> 0.301</td> <td>-5.66e+04</td> <td> 1.75e+04</td>
    </tr>
    <tr>
      <th>Longtitude_miss</th>    <td>-1.956e+04</td> <td> 1.89e+04</td> <td>   -1.034</td> <td> 0.301</td> <td>-5.66e+04</td> <td> 1.75e+04</td>
    </tr>
    <tr>
      <th>Regionname_miss</th>    <td>-4.717e+04</td> <td> 1.12e+05</td> <td>   -0.421</td> <td> 0.674</td> <td>-2.67e+05</td> <td> 1.72e+05</td>
    </tr>
    <tr>
      <th>Propertycount_miss</th> <td>-4.717e+04</td> <td> 1.12e+05</td> <td>   -0.421</td> <td> 0.674</td> <td>-2.67e+05</td> <td> 1.72e+05</td>
    </tr>
    </table>
    <table class="simpletable">
    <tr>
      <th>Omnibus:</th>       <td>13579.846</td> <th>  Durbin-Watson:     </th>  <td>   2.004</td> 
    </tr>
    <tr>
      <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>485785.597</td>
    </tr>
    <tr>
      <th>Skew:</th>           <td> 2.698</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> 
    </tr>
    <tr>
      <th>Kurtosis:</th>       <td>26.269</td>   <th>  Cond. No.          </th>  <td>1.20e+16</td> 
    </tr>
    </table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.21e-20. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular.



There seem to be many strong predictors in the OLS, particularly,
variables like Rooms, Distance, Cars, Bathroom, but also the missingness
indicators seem to have some predictive power. Interestingly the postal
code is also predictive in a linear model, even though using the postal
code as a numerical variable in linear regression does not make much
sense, although nearby postal codes do tend to be close geographically.

Regardless, because we are doing a train-test split, we can feel
confident that the resulting test error is a good estimate of the risk
of the prediction.

.. code:: python

    ols_error = np.mean((Y_pred - Y_te)**2)
    ols_error / CA_error




.. parsed-literal::

    0.7162549957690686



We see that the test error from the OLS is 71.6% of the prediction from
the CouncilArea, and we can confidently use the OLS prediction to impute
the missing Prices.

.. code:: python

    Y, X = select_preds(mel[~mel['Price_miss']])
    _, X_miss = select_preds(mel[mel['Price_miss']])
    ols_res = sm.OLS(Y,X).fit()
    Y_pred = ols_res.predict(X_miss)
    mel.loc[mel['Price_miss'],'Price'] = Y_pred

We have used more basic Pandas indexing and assignment to do the
imputation above. Try to follow the precise operations we have
performed. As usual, we can look at the sample distribution for the
resulting imputation.

.. code:: python

    mel['Price'].hist(bins=40)
    plt.title("Histogram of Price: imputed with OLS")
    plt.show()



.. image:: output_89_0.png

