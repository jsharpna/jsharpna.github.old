<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>A First Learning Machine &#8212; DataTech 1.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Data and Computation" href="data.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="a-first-learning-machine">
<h1>A First Learning Machine<a class="headerlink" href="#a-first-learning-machine" title="Permalink to this headline">¶</a></h1>
<p>In this chapter, we will see the following:</p>
<ul class="simple">
<li>Supervised learning</li>
<li>Numpy and vectorization</li>
<li>Our first learning machine: the perceptron</li>
</ul>
<div class="section" id="supervised-learning">
<h2>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">## Numpy is loaded with namespace prefix np</span>
<span class="c1">## To access anything in numpy you need to prepend it with np</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">data_folder</span> <span class="o">=</span> <span class="s1">&#39;../data/&#39;</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_folder</span> <span class="o">+</span> <span class="s1">&#39;winequality-red.csv&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">winefile</span><span class="p">:</span>
    <span class="n">header</span> <span class="o">=</span> <span class="n">winefile</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
    <span class="n">wine_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;;&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">winefile</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">wine_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;7.4&#39;</span><span class="p">,</span> <span class="s1">&#39;0.7&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1.9&#39;</span><span class="p">,</span> <span class="s1">&#39;0.076&#39;</span><span class="p">,</span> <span class="s1">&#39;11&#39;</span><span class="p">,</span> <span class="s1">&#39;34&#39;</span><span class="p">,</span> <span class="s1">&#39;0.9978&#39;</span><span class="p">,</span> <span class="s1">&#39;3.51&#39;</span><span class="p">,</span> <span class="s1">&#39;0.56&#39;</span><span class="p">,</span> <span class="s1">&#39;9.4&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">wine_ar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_list</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">wine_ar</span><span class="p">)</span>
<span class="n">wine_ar</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[[</span> <span class="mf">7.4</span>    <span class="mf">0.7</span>    <span class="mf">0.</span>    <span class="o">...</span>  <span class="mf">0.56</span>   <span class="mf">9.4</span>    <span class="mf">5.</span>   <span class="p">]</span>
 <span class="p">[</span> <span class="mf">7.8</span>    <span class="mf">0.88</span>   <span class="mf">0.</span>    <span class="o">...</span>  <span class="mf">0.68</span>   <span class="mf">9.8</span>    <span class="mf">5.</span>   <span class="p">]</span>
 <span class="p">[</span> <span class="mf">7.8</span>    <span class="mf">0.76</span>   <span class="mf">0.04</span>  <span class="o">...</span>  <span class="mf">0.65</span>   <span class="mf">9.8</span>    <span class="mf">5.</span>   <span class="p">]</span>
 <span class="o">...</span>
 <span class="p">[</span> <span class="mf">6.3</span>    <span class="mf">0.51</span>   <span class="mf">0.13</span>  <span class="o">...</span>  <span class="mf">0.75</span>  <span class="mf">11.</span>     <span class="mf">6.</span>   <span class="p">]</span>
 <span class="p">[</span> <span class="mf">5.9</span>    <span class="mf">0.645</span>  <span class="mf">0.12</span>  <span class="o">...</span>  <span class="mf">0.71</span>  <span class="mf">10.2</span>    <span class="mf">5.</span>   <span class="p">]</span>
 <span class="p">[</span> <span class="mf">6.</span>     <span class="mf">0.31</span>   <span class="mf">0.47</span>  <span class="o">...</span>  <span class="mf">0.66</span>  <span class="mf">11.</span>     <span class="mf">6.</span>   <span class="p">]]</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1599</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
<p>1599 records with 12 rows, we want to predict the quality which is the
last column with the characteristics of the wine (first 11 columns).</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">header</span>
</pre></div>
</div>
<pre class="literal-block">
'&quot;fixed acidity&quot;;&quot;volatile acidity&quot;;&quot;citric acid&quot;;&quot;residual sugar&quot;;&quot;chlorides&quot;;&quot;free sulfur dioxide&quot;;&quot;total sulfur dioxide&quot;;&quot;density&quot;;&quot;pH&quot;;&quot;sulphates&quot;;&quot;alcohol&quot;;&quot;quality&quot;n'
</pre>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">header</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;;&#39;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;fixed acidity&#39;</span><span class="p">,</span> <span class="s1">&#39;volatile acidity&#39;</span><span class="p">,</span> <span class="s1">&#39;citric acid&#39;</span><span class="p">,</span> <span class="s1">&#39;residual sugar&#39;</span><span class="p">,</span> <span class="s1">&#39;chlorides&#39;</span><span class="p">,</span> <span class="s1">&#39;free sulfur dioxide&#39;</span><span class="p">,</span> <span class="s1">&#39;total sulfur dioxide&#39;</span><span class="p">,</span> <span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="s1">&#39;pH&#39;</span><span class="p">,</span> <span class="s1">&#39;sulphates&#39;</span><span class="p">,</span> <span class="s1">&#39;alcohol&#39;</span><span class="p">,</span> <span class="s1">&#39;quality&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#Subselect the predictor X and response y</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">wine_ar</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">wine_ar</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">n</span><span class="p">,</span><span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="c1">#just checking</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">((</span><span class="mi">1599</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1599</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="predicting-wine-quality-with-ordinary-least-squares-ols">
<h3>Predicting wine quality with Ordinary Least Squares (OLS)<a class="headerlink" href="#predicting-wine-quality-with-ordinary-least-squares-ols" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">X</span><span class="p">))</span> <span class="c1">#add intercept</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">wine_ols</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1">#Initialize the OLS</span>
<span class="n">wine_res</span> <span class="o">=</span> <span class="n">wine_ols</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">wine_res</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span>                            <span class="n">OLS</span> <span class="n">Regression</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>                      <span class="n">y</span>   <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                       <span class="mf">0.361</span>
<span class="n">Model</span><span class="p">:</span>                            <span class="n">OLS</span>   <span class="n">Adj</span><span class="o">.</span> <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                  <span class="mf">0.356</span>
<span class="n">Method</span><span class="p">:</span>                 <span class="n">Least</span> <span class="n">Squares</span>   <span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span>                     <span class="mf">81.35</span>
<span class="n">Date</span><span class="p">:</span>                <span class="n">Thu</span><span class="p">,</span> <span class="mi">06</span> <span class="n">Sep</span> <span class="mi">2018</span>   <span class="n">Prob</span> <span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span>          <span class="mf">1.79e-145</span>
<span class="n">Time</span><span class="p">:</span>                        <span class="mi">11</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">16</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">1569.1</span>
<span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>                <span class="mi">1599</span>   <span class="n">AIC</span><span class="p">:</span>                             <span class="mf">3162.</span>
<span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                    <span class="mi">1587</span>   <span class="n">BIC</span><span class="p">:</span>                             <span class="mf">3227.</span>
<span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                          <span class="mi">11</span>
<span class="n">Covariance</span> <span class="n">Type</span><span class="p">:</span>            <span class="n">nonrobust</span>
<span class="o">==============================================================================</span>
                 <span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">t</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span>      <span class="p">[</span><span class="mf">0.025</span>      <span class="mf">0.975</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="n">const</span>         <span class="mf">21.9652</span>     <span class="mf">21.195</span>      <span class="mf">1.036</span>      <span class="mf">0.300</span>     <span class="o">-</span><span class="mf">19.607</span>      <span class="mf">63.538</span>
<span class="n">x1</span>             <span class="mf">0.0250</span>      <span class="mf">0.026</span>      <span class="mf">0.963</span>      <span class="mf">0.336</span>      <span class="o">-</span><span class="mf">0.026</span>       <span class="mf">0.076</span>
<span class="n">x2</span>            <span class="o">-</span><span class="mf">1.0836</span>      <span class="mf">0.121</span>     <span class="o">-</span><span class="mf">8.948</span>      <span class="mf">0.000</span>      <span class="o">-</span><span class="mf">1.321</span>      <span class="o">-</span><span class="mf">0.846</span>
<span class="n">x3</span>            <span class="o">-</span><span class="mf">0.1826</span>      <span class="mf">0.147</span>     <span class="o">-</span><span class="mf">1.240</span>      <span class="mf">0.215</span>      <span class="o">-</span><span class="mf">0.471</span>       <span class="mf">0.106</span>
<span class="n">x4</span>             <span class="mf">0.0163</span>      <span class="mf">0.015</span>      <span class="mf">1.089</span>      <span class="mf">0.276</span>      <span class="o">-</span><span class="mf">0.013</span>       <span class="mf">0.046</span>
<span class="n">x5</span>            <span class="o">-</span><span class="mf">1.8742</span>      <span class="mf">0.419</span>     <span class="o">-</span><span class="mf">4.470</span>      <span class="mf">0.000</span>      <span class="o">-</span><span class="mf">2.697</span>      <span class="o">-</span><span class="mf">1.052</span>
<span class="n">x6</span>             <span class="mf">0.0044</span>      <span class="mf">0.002</span>      <span class="mf">2.009</span>      <span class="mf">0.045</span>       <span class="mf">0.000</span>       <span class="mf">0.009</span>
<span class="n">x7</span>            <span class="o">-</span><span class="mf">0.0033</span>      <span class="mf">0.001</span>     <span class="o">-</span><span class="mf">4.480</span>      <span class="mf">0.000</span>      <span class="o">-</span><span class="mf">0.005</span>      <span class="o">-</span><span class="mf">0.002</span>
<span class="n">x8</span>           <span class="o">-</span><span class="mf">17.8812</span>     <span class="mf">21.633</span>     <span class="o">-</span><span class="mf">0.827</span>      <span class="mf">0.409</span>     <span class="o">-</span><span class="mf">60.314</span>      <span class="mf">24.551</span>
<span class="n">x9</span>            <span class="o">-</span><span class="mf">0.4137</span>      <span class="mf">0.192</span>     <span class="o">-</span><span class="mf">2.159</span>      <span class="mf">0.031</span>      <span class="o">-</span><span class="mf">0.789</span>      <span class="o">-</span><span class="mf">0.038</span>
<span class="n">x10</span>            <span class="mf">0.9163</span>      <span class="mf">0.114</span>      <span class="mf">8.014</span>      <span class="mf">0.000</span>       <span class="mf">0.692</span>       <span class="mf">1.141</span>
<span class="n">x11</span>            <span class="mf">0.2762</span>      <span class="mf">0.026</span>     <span class="mf">10.429</span>      <span class="mf">0.000</span>       <span class="mf">0.224</span>       <span class="mf">0.328</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span>                       <span class="mf">27.376</span>   <span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span>                   <span class="mf">1.757</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span>                  <span class="mf">0.000</span>   <span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span> <span class="p">(</span><span class="n">JB</span><span class="p">):</span>               <span class="mf">40.965</span>
<span class="n">Skew</span><span class="p">:</span>                          <span class="o">-</span><span class="mf">0.168</span>   <span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span>                     <span class="mf">1.27e-09</span>
<span class="n">Kurtosis</span><span class="p">:</span>                       <span class="mf">3.708</span>   <span class="n">Cond</span><span class="o">.</span> <span class="n">No</span><span class="o">.</span>                     <span class="mf">1.13e+05</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="n">Standard</span> <span class="n">Errors</span> <span class="n">assume</span> <span class="n">that</span> <span class="n">the</span> <span class="n">covariance</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">the</span> <span class="n">errors</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">specified</span><span class="o">.</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="n">The</span> <span class="n">condition</span> <span class="n">number</span> <span class="ow">is</span> <span class="n">large</span><span class="p">,</span> <span class="mf">1.13e+05</span><span class="o">.</span> <span class="n">This</span> <span class="n">might</span> <span class="n">indicate</span> <span class="n">that</span> <span class="n">there</span> <span class="n">are</span>
<span class="n">strong</span> <span class="n">multicollinearity</span> <span class="ow">or</span> <span class="n">other</span> <span class="n">numerical</span> <span class="n">problems</span><span class="o">.</span>
</pre></div>
</div>
<p>There are a few things to notice: - the R-squared statistic is pretty
poor, which means that the addition of these covariates is not reducing
the residual variance by much - there is strong colinearity, which means
that some of these predictor variables can be nearly written as a linear
combination of the others</p>
</div>
</div>
<div class="section" id="prediction-evaluation-with-training-test-split">
<h2>Prediction evaluation with training-test split<a class="headerlink" href="#prediction-evaluation-with-training-test-split" title="Permalink to this headline">¶</a></h2>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">wine_ar</span><span class="p">)</span> <span class="c1">#start by randomly shuffling data</span>
<span class="n">wine_ar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">wine_ar</span><span class="p">))</span> <span class="c1">#add intercept</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">2</span> <span class="c1">#set training set size 50-50 split</span>
<span class="n">wine_tr</span><span class="p">,</span> <span class="n">wine_te</span> <span class="o">=</span> <span class="n">wine_ar</span><span class="p">[:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">wine_ar</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span> <span class="c1"># training and test split</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1"># fit data and get the estimated beta</span>
<span class="n">train_ols</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">wine_tr</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">wine_tr</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">train_res</span> <span class="o">=</span> <span class="n">train_ols</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">train_res</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">y_te</span><span class="p">,</span> <span class="n">X_te</span> <span class="o">=</span> <span class="n">wine_te</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">wine_te</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X_te</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_te</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span> <span class="c1">#extract sq error losses</span>
<span class="n">te_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="c1">#average losses</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">te_error</span><span class="p">,</span> <span class="n">te_error</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1">#test error and its sqrt</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.41531101266937576</span> <span class="mf">0.6444462837734234</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">y_const</span> <span class="o">=</span> <span class="n">wine_te</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1">#what if we predict with a single value?</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_te</span> <span class="o">-</span> <span class="n">y_const</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">)</span> <span class="c1">#test error - do you see the array broadcasting?</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.6407359375</span>
</pre></div>
</div>
<p>The test error for the OLS regression is 0.415 but we should compare
this to the test error for the constant predictor which is 0.640. This
is consistent with the small R-squared in the above analysis.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span> <span class="c1">#predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_te</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])</span> <span class="c1">#actual values</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">5.28516483</span> <span class="mf">5.20432846</span> <span class="mf">5.37849799</span> <span class="mf">6.44910839</span> <span class="mf">6.03562429</span> <span class="mf">6.149762</span>
 <span class="mf">5.12044022</span> <span class="mf">5.85733763</span> <span class="mf">6.0109202</span>  <span class="mf">6.1063934</span> <span class="p">]</span>
<span class="p">[</span><span class="mf">5.</span> <span class="mf">4.</span> <span class="mf">5.</span> <span class="mf">7.</span> <span class="mf">6.</span> <span class="mf">6.</span> <span class="mf">6.</span> <span class="mf">5.</span> <span class="mf">6.</span> <span class="mf">6.</span><span class="p">]</span>
</pre></div>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">y_round</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1">#what if we needed to predict an integer? round!</span>
<span class="n">ro_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_te</span> <span class="o">-</span> <span class="n">y_round</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ro_error</span><span class="p">,</span> <span class="n">ro_error</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.50625</span> <span class="mf">0.7115124735378854</span>
</pre></div>
</div>
<p>The square error loss is worse if we round the prediction to the nearest
integer, which shouldn&#8217;t be too surprising. We lose flexibility to
minimize the square error loss if we are restricted to integers.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_round</span> <span class="o">!=</span> <span class="n">y_te</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.41625</span>
</pre></div>
</div>
<p>One advantage of rounding is if we modify our loss function to be the
0-1 loss, then we have a chance of doing better. The average 0-1 loss is
the mis-classification rate and it is 41.6%.</p>
<p>A supervised learner is any algorithm that takes p dimensional training data of the form</p>
<div class="math">
\[x_i \in \mathbb R^p, y_i \in \mathbb R, \quad i=1,\ldots,n\]</div>
<p>and fits a learner, which is a function that predicts y from a new datapoint, or a test point x, which we denote <span class="math">\(\hat f(x)\)</span>.
The corresponding random variables <span class="math">\(X,Y\)</span> are called the predictor and response variables respectively, and we will assume that the X variables are p dimensional.</p>
<p>For example, we are a car insurance company and we want to predict how much a customer will cost our company in claims per year.
Before a customer takes out a policy they fill out a form to obtain a quote for the cost of the policy (the premium).
This form tells the company certain details about the customer, such as where they live, demographic information, and driver history.</p>
<p>Linear regression is an example of a supervised learner such that</p>
<div class="math">
\[\hat f(x) = \sum_{j = 1}^p \hat \beta_j x_j = \hat \beta^\top x\]</div>
<p>EXAMPLE OF PREDICT ON ABOVE DATA WITH LINEAR REGRESSION</p>
<p>CLASSIFICATION</p>
<p>BATCH V ONLINE LEARNING In machine learning, we call a method <em>online</em> if it will take streaming training data, where the data</p>
</div>
<div class="section" id="numpy-and-vectorization">
<h2>Numpy and Vectorization<a class="headerlink" href="#numpy-and-vectorization" title="Permalink to this headline">¶</a></h2>
<p>INTRO NUMPY AND LINEAR ALGEBRA</p>
<p>SLICING</p>
<p>REWRITE ABOVE CODE IN NUMPY</p>
<p>VECTORIZATION AND MATRIX MULTIPLY</p>
<p>BROADCASTING FOR TEST PREDICTION</p>
</div>
<div class="section" id="the-perceptron">
<h2>The Perceptron<a class="headerlink" href="#the-perceptron" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">A First Learning Machine</a><ul>
<li><a class="reference internal" href="#supervised-learning">Supervised Learning</a><ul>
<li><a class="reference internal" href="#predicting-wine-quality-with-ordinary-least-squares-ols">Predicting wine quality with Ordinary Least Squares (OLS)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prediction-evaluation-with-training-test-split">Prediction evaluation with training-test split</a></li>
<li><a class="reference internal" href="#numpy-and-vectorization">Numpy and Vectorization</a></li>
<li><a class="reference internal" href="#the-perceptron">The Perceptron</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="data.html" title="previous chapter">Data and Computation</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/unit1/perceptron.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, James Sharpnack.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
      |
      <a href="../_sources/unit1/perceptron.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>